{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12958428-4d43-4cb4-85e1-3655872fdecf",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "## Part 1: Manual Hyperparameter Tuning\n",
    "### Objective\n",
    "Manually tune hyperparameters of a neural network and observe the impact on model performance.\n",
    "\n",
    "### Setup\n",
    "Start with the necessary imports and dataset preparation. We'll use the MNIST dataset for this exercise, as it's complex enough to demonstrate the effects of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "660f35c7-8216-4090-8254-232038418180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and preprocess the MNIST dataset,which consists of 28x28 pixel grayscale images of handwritten digits (0 through 9) along with their corresponding labels\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Reshape and Normalize the image data\n",
    "X_train, X_test = X_train.reshape(-1, 784) / 255.0, X_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "#One-Hot Encode the labels \n",
    "#One-hot encoding is a binary matrix representation of the labels. \n",
    "#For example, the label '3' would be converted to [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],which is 10 in the case of the MNIST dataset (digits 0 through 9).\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df2a6d-419d-4f89-8c8d-bbf3ee27617b",
   "metadata": {},
   "source": [
    "## Task: Manual Tuning of Hyperparameters\n",
    "1. Build a Base Model: Create a simple neural network as a starting point.\n",
    "2. Manual Tuning: Experiment by manually changing hyperparameters like learning rate, number of layers/neurons, and activation functions.\n",
    "3. Training and Evaluation: Train the model with different hyperparameter settings and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84ec10a6-9de9-4bee-9212-012d04f1dd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\vinee\\OneDrive\\Dokument\\Github\\Deep-Learning\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\vinee\\OneDrive\\Dokument\\Github\\Deep-Learning\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def build_model(hyperparams):\n",
    "    # Construct a model based on hyperparams\n",
    "    \n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    layer_neuron1, layer_neuron2 = hyperparams['layer_neurons']\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(layer_neuron1, activation='relu', input_shape=(784,)),\n",
    "        tf.keras.layers.Dense(layer_neuron2, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)   \n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "  \n",
    "\n",
    "# Example hyperparameters to tune\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "layer_configs = [(32, 32), (64, 64), (128, 128)]\n",
    "\n",
    "losses = []\n",
    "acurracies= []\n",
    "# Loop through different hyperparameters and train models\n",
    "for lr in learning_rates:\n",
    "    for layers in layer_configs:\n",
    "        hyperparams = {'learning_rate': lr, 'layer_neurons': layers}\n",
    "        # Build and train your model\n",
    "        model = build_model(hyperparams)\n",
    "        history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=0)\n",
    "        acurracies.append(history.history['accuracy'])\n",
    "        losses.append(history.history['loss'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e2102-d8a6-459f-a9a5-b9d756090c83",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Plot the accuracy and loss for different hyperparameter settings.\n",
    "\n",
    "## Analysis and Questions\n",
    "* How did different learning rates affect the training process and model accuracy?\n",
    "* What impact did varying the number of layers and neurons have on the model's performance?\n",
    "* Were there any combinations of hyperparameters that resulted in particularly good or poor performance?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, layers in enumerate(layer_configs):\n",
    "        label = f'LR={lr}, Layers={layers}'\n",
    "        plt.plot(acurracies[i * len(layer_configs) + j], label=label)\n",
    "\n",
    "plt.title('Accuracy for Different Hyperparameter Settings')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "        \n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    for j, layers in enumerate(layer_configs):\n",
    "        label = f'LR={lr}, Layers={layers}'\n",
    "        plt.plot(losses[i * len(layer_configs) + j], label=label)\n",
    "\n",
    "plt.title('Loss for Different Hyperparameter Settings')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd49c263-b865-4aef-a4c0-10b337300430",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "## Part 2: Automated Hyperparameter Tuning\n",
    "### Objective\n",
    "Use automated methods like Grid Search and Random Search for hyperparameter tuning.\n",
    "\n",
    "### Setup\n",
    "Reuse the MNIST dataset setup from Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f58055-b61f-4e4c-8358-fc6d8121874e",
   "metadata": {},
   "source": [
    "### Task: Automated Hyperparameter Tuning\n",
    "1. Grid Search and Random Search: Introduce and apply Grid Search and Random Search using scikit-learn's GridSearchCV or RandomizedSearchCV.\n",
    "2. Integration with Keras: Show how to use these methods with Keras models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe24fe-a682-4835-a788-06776264f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Define a function to create a model (for KerasClassifier)\n",
    "def create_model_to_search(hyperparams):\n",
    "    # Create a Keras model with hyperparameters\n",
    "    pass  # Replace with your code\n",
    "\n",
    "# Set up GridSearchCV or RandomizedSearchCV\n",
    "model_to_search = KerasClassifier(build_fn=create_model_to_search)\n",
    "param_grid = {\n",
    "    # Define a grid of hyperparameters to search\n",
    "}\n",
    "grid = GridSearchCV(estimator=model_to_search, param_grid=param_grid)\n",
    "\n",
    "# Run grid search\n",
    "grid_result = grid.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3550919-24ac-4001-a2a0-1e0bb583adbc",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Visualize the performance of the best model found by the search methods.\n",
    "## Analysis and Questions\n",
    "* Compare the results of manual tuning with automated tuning. Which method gave better results?\n",
    "* What are the advantages and limitations of using automated methods like Grid Search and Random Search?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73a6e8f-a9bc-4ed8-85d6-71ade7920ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
